# NYC Taxi Demand & Tipping Patterns

## 1. Project overview

Analyze NYC Yellow Taxi trip records to understand spatial/temporal demand patterns and tipping behavior. Store raw and processed data in a local S3-compatible bucket (MinIO) inside Docker. Run notebooks and PySpark inside the same container stack to ingest, process, analyze, visualize, and model demand and tip prediction.

## 2. Streamlit dashboards (live)

The following Streamlit dashboards are deployed and available:

* **Nearby Hotspots Finder** — https://nyc-taxi-nearby-hotspots-finder.streamlit.app/

* **NYC Taxi Tip Estimator** — https://nyc-taxi-tip-estimator.streamlit.app/

* **Taxi Tip Amount Predictor and Model Performance** — https://taxi-tip-model-performance.streamlit.app/

Use these links to view the interactive dashboards. They read processed outputs generated by the notebooks / ETL.

## 3. Primary objectives

* Ingest multi-month/year NYC taxi trip data into a local S3 (MinIO).

* Clean & preprocess large Parquet files efficiently with PySpark.

* **Exploratory Data Analysis (EDA):** Daily/weekly/hourly demand, geographic hot-spots, trip distance/time distributions, and tip distributions.

* **Tipping patterns:** Correlate tip amount / tip percentage with fare, payment method, time-of-day, day-of-week, pickup/dropoff location, passenger count.

* **Demand forecasting:** Time-series forecast (per-zone or aggregated) to predict short-term demand.

* **Tip prediction (optional ML):** Predict likelihood and amount/percentage of tipping for a trip.

* **Deliverables:** Notebooks, reproducible Docker setup, sample dashboards/visualizations (Plotly/Matplotlib/nbviews), and documentation.

## 4. Scope (what’s included/optional)

**Included:**

* NYC Yellow taxi dataset ingestion.
* Local S3 (MinIO) for object storage.
* PySpark ETL & analysis in Jupyter notebooks.
* EDA visualizations, nearby hotspots recommendation, and tip analysis and prediction.

## 5. High-level architecture

**Docker Compose services:**

* Minio (S3-compatible object store)
* Jupyter (JupyterLab or Notebook with PySpark + required libs)

**Data flow:**

* Place Parquet files locally or upload to MinIO.
* PySpark in Jupyter reads parquet files from S3 endpoints (MinIO).
* Write processed results back to S3 for persistence.
* Visualizations are generated in notebooks or through Tableau.

## 6. Deliverables

* Dockerized environment (docker-compose.yaml).
* Jupyter notebooks for each process.
* Processed parquet files in S3 layout.
* Interactive dashboards (Streamlit) reading outputs from reduced datasets for faster processing.
