{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9193c95",
   "metadata": {},
   "source": [
    "# Tip Range Prediction Model (Classification)\n",
    "\n",
    "This notebook implements a classification model to predict the **range** of the tip amount.\n",
    "\n",
    "## Classes (Target Bins):\n",
    "*   **0 (No Tip)**: tip = 0\n",
    "*   **1 (Low)**: 0 < tip <= 3\n",
    "*   **2 (Medium)**: 3 < tip <= 6\n",
    "*   **3 (High)**: 6 < tip <= 10\n",
    "*   **4 (Very High)**: tip > 10\n",
    "\n",
    "## Models:\n",
    "1. Random Forest Classifier\n",
    "2. XGBoost Classifier\n",
    "3. Random Forest Classifier with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1918dda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, Bucketizer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from xgboost.spark import SparkXGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d02f0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Config loaded\n",
      "ℹ No existing Spark session to stop\n",
      "Creating Spark session: NYC Taxi EDA\n",
      "✓ Spark session created successfully (version 3.5.0)\n",
      "Configuring Hadoop for MinIO...\n",
      "✓ Spark configured for environment: development\n",
      "✓ Using MinIO endpoint: http://minio:9000\n",
      "✓ Reading from bucket: nyc-taxi\n",
      "\n",
      "==================================================\n",
      "✓ Spark Session Ready\n",
      "==================================================\n",
      "==================================================\n",
      "Current Configuration:\n",
      "==================================================\n",
      "App Name: NYC Taxi EDA\n",
      "Environment: development\n",
      "MinIO Endpoint: http://minio:9000\n",
      "S3 Bucket: nyc-taxi\n",
      "Spark Driver Memory: 3g\n",
      "Spark Executor Memory: 3g\n",
      "Spark Executor Instances: 3\n",
      "Spark Executor Cores: 2\n",
      "Log Level: INFO\n",
      "Log File: eda.log\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import importlib.util\n",
    "import os\n",
    "\n",
    "# ========== LOAD CONFIG FIRST ==========\n",
    "src_path = os.path.join(os.path.dirname(os.getcwd()), 'src')\n",
    "config_file = os.path.join(src_path, 'config.py')\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"config\", config_file)\n",
    "config_module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(config_module)\n",
    "\n",
    "Config = config_module.Config\n",
    "print(\"✓ Config loaded\")\n",
    "\n",
    "# ========== STOP EXISTING SPARK ==========\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"✓ Stopped existing Spark session\")\n",
    "except:\n",
    "    print(\"ℹ No existing Spark session to stop\")\n",
    "\n",
    "# ========== CREATE SPARK SESSION ==========\n",
    "print(f\"Creating Spark session: {Config.APP_NAME}\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(Config.APP_NAME) \\\n",
    "    .config(\"spark.driver.memory\", Config.SPARK_DRIVER_MEMORY) \\\n",
    "    .config(\"spark.executor.memory\", Config.SPARK_EXECUTOR_MEMORY) \\\n",
    "    .config(\"spark.executor.instances\", Config.SPARK_EXECUTOR_INSTANCES) \\\n",
    "    .config(\"spark.executor.cores\", Config.SPARK_EXECUTOR_CORES) \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✓ Spark session created successfully (version {spark.version})\")\n",
    "\n",
    "# ========== CONFIGURE HADOOP FOR MINIO ==========\n",
    "print(\"Configuring Hadoop for MinIO...\")\n",
    "\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", Config.MINIO_ENDPOINT)\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", Config.MINIO_ACCESS_KEY)\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", Config.MINIO_SECRET_KEY)\n",
    "hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoop_conf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "print(f\"✓ Spark configured for environment: {Config.ENVIRONMENT}\")\n",
    "print(f\"✓ Using MinIO endpoint: {Config.MINIO_ENDPOINT}\")\n",
    "print(f\"✓ Reading from bucket: {Config.S3_BUCKET_NAME}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"✓ Spark Session Ready\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Display configuration\n",
    "Config.display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6667fe47",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7158c704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 7975663\n"
     ]
    }
   ],
   "source": [
    "data_path = \"s3a://nyc-taxi/Tip_Prediction_Model_DF/\"\n",
    "df = spark.read.parquet(data_path)\n",
    "print(f\"Total records: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d5fae9",
   "metadata": {},
   "source": [
    "## Feature Engineering & Target Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "875d8e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|  0.0| 430738|\n",
      "|  1.0|2863423|\n",
      "|  2.0|3283119|\n",
      "|  3.0| 654426|\n",
      "|  4.0| 713790|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop missing values\n",
    "df = df.dropna(subset=[\n",
    "    \"fare_amount\", \"trip_distance\", \"payment_type\",\n",
    "    \"tip_amount\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"\n",
    "])\n",
    "\n",
    "# 1. Time Features\n",
    "df = df.withColumn(\"pickup_hour\", F.hour(\"tpep_pickup_datetime\")) \\\n",
    "       .withColumn(\"pickup_day\", F.dayofweek(\"tpep_pickup_datetime\")) \\\n",
    "       .withColumn(\"pickup_month\", F.month(\"tpep_pickup_datetime\")) \\\n",
    "       .withColumn(\"is_weekend\", F.when(F.col(\"pickup_day\").isin([1, 7]), 1).otherwise(0))\n",
    "\n",
    "# 2. Trip Duration\n",
    "df = df.withColumn(\n",
    "    \"trip_duration\",\n",
    "    (F.unix_timestamp(\"tpep_dropoff_datetime\") - F.unix_timestamp(\"tpep_pickup_datetime\")) / 60\n",
    ").filter((F.col(\"trip_duration\") > 0) & (F.col(\"trip_duration\") < 300))\n",
    "\n",
    "# 3. Target Binning (Tip Class)\n",
    "# 0: No Tip (0)\n",
    "# 1: Low (0 - 3]\n",
    "# 2: Medium (3 - 6]\n",
    "# 3: High (6 - 10]\n",
    "# 4: Very High (> 10)\n",
    "\n",
    "bucketizer = Bucketizer(\n",
    "    splits=[-float(\"inf\"), 0.0001, 3, 6, 10, float(\"inf\")],\n",
    "    inputCol=\"tip_amount\",\n",
    "    outputCol=\"tip_class_raw\"\n",
    ")\n",
    "\n",
    "# Bucketizer outputs double, we cast to int (and handle 0 case separately if needed, but 0.0001 split handles 0)\n",
    "# Actually, exact 0 needs to be its own class. \n",
    "# Range: (-inf, 0.0001) -> Bucket 0 (Includes 0)\n",
    "# Range: [0.0001, 3) -> Bucket 1\n",
    "# ...\n",
    "df = bucketizer.transform(df)\n",
    "df = df.withColumn(\"label\", F.col(\"tip_class_raw\").cast(\"double\"))\n",
    "\n",
    "# Check class distribution\n",
    "df.groupBy(\"label\").count().orderBy(\"label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d7a024",
   "metadata": {},
   "source": [
    "## Vector Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a295cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"payment_type\", outputCol=\"payment_type_idx\", handleInvalid=\"keep\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "feature_cols = [\n",
    "    \"fare_amount\", \"trip_distance\", \"trip_duration\",\n",
    "    \"pickup_hour\", \"pickup_day\", \"pickup_month\",\n",
    "    \"is_weekend\", \"payment_type_idx\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_ml = assembler.transform(df).select(\"features\", \"label\")\n",
    "\n",
    "train_df, test_df = df_ml.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ce468a",
   "metadata": {},
   "source": [
    "## 1. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b43ed7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.7134076066077603\n",
      "Random Forest F1 Score: 0.6920054707191383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=20,\n",
    "    maxDepth=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_model = rf.fit(train_df)\n",
    "rf_preds = rf_model.transform(test_df)\n",
    "\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "print(f\"Random Forest Accuracy: {evaluator_acc.evaluate(rf_preds)}\")\n",
    "print(f\"Random Forest F1 Score: {evaluator_f1.evaluate(rf_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b8da0",
   "metadata": {},
   "source": [
    "## 2. XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a5ee8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 15:46:18,989 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n",
      "\tbooster params: {'objective': 'multi:softprob', 'device': 'cpu', 'num_class': 5, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2026-01-29 15:46:21,455 INFO XGBoost-PySpark: _train_booster Training on CPUs 2]\n",
      "[15:46:22] Task 1 got rank 1[15:46:22] Task 0 got rank 0\n",
      "\n",
      "2026-01-29 15:46:28,809 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2026-01-29 15:46:29,052 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.7436627346756265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 15:46:30,901 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost F1 Score: 0.7325345050544245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "xgb = SparkXGBClassifier(\n",
    "    features_col=\"features\",\n",
    "    label_col=\"label\",\n",
    "    num_workers=2,\n",
    "    use_gpu=False\n",
    ")\n",
    "\n",
    "xgb_model = xgb.fit(train_df)\n",
    "xgb_preds = xgb_model.transform(test_df)\n",
    "\n",
    "print(f\"XGBoost Accuracy: {evaluator_acc.evaluate(xgb_preds)}\")\n",
    "print(f\"XGBoost F1 Score: {evaluator_f1.evaluate(xgb_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16136524",
   "metadata": {},
   "source": [
    "## 3. Random Forest Classifier with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42d2dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/29 15:40:01 WARN DAGScheduler: Broadcasting large task binary with size 1232.5 KiB\n",
      "26/01/29 15:40:02 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "26/01/29 15:40:05 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "26/01/29 15:40:07 WARN DAGScheduler: Broadcasting large task binary with size 1267.1 KiB\n",
      "26/01/29 15:40:08 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
      "26/01/29 15:40:09 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "26/01/29 15:40:11 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n",
      "26/01/29 15:40:20 WARN DAGScheduler: Broadcasting large task binary with size 1224.7 KiB\n",
      "26/01/29 15:40:22 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "26/01/29 15:40:24 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "26/01/29 15:40:27 WARN DAGScheduler: Broadcasting large task binary with size 1361.7 KiB\n",
      "26/01/29 15:40:27 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "26/01/29 15:40:30 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "26/01/29 15:40:30 WARN DAGScheduler: Broadcasting large task binary with size 15.8 MiB\n",
      "26/01/29 15:40:35 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "26/01/29 15:40:36 WARN DAGScheduler: Broadcasting large task binary with size 9.5 MiB\n",
      "26/01/29 15:40:45 WARN DAGScheduler: Broadcasting large task binary with size 1234.4 KiB\n",
      "26/01/29 15:40:46 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "26/01/29 15:40:49 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "26/01/29 15:40:50 WARN DAGScheduler: Broadcasting large task binary with size 1274.8 KiB\n",
      "26/01/29 15:40:51 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
      "26/01/29 15:40:54 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "26/01/29 15:40:56 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "26/01/29 15:41:03 WARN DAGScheduler: Broadcasting large task binary with size 1223.7 KiB\n",
      "26/01/29 15:41:04 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "26/01/29 15:41:07 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "26/01/29 15:41:09 WARN DAGScheduler: Broadcasting large task binary with size 1370.6 KiB\n",
      "26/01/29 15:41:10 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "26/01/29 15:41:13 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "26/01/29 15:41:13 WARN DAGScheduler: Broadcasting large task binary with size 15.9 MiB\n",
      "26/01/29 15:41:18 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "26/01/29 15:41:20 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n",
      "26/01/29 15:41:30 WARN DAGScheduler: Broadcasting large task binary with size 1236.7 KiB\n",
      "26/01/29 15:41:31 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "26/01/29 15:41:34 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "26/01/29 15:41:35 WARN DAGScheduler: Broadcasting large task binary with size 1276.3 KiB\n",
      "26/01/29 15:41:36 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
      "26/01/29 15:41:40 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "26/01/29 15:41:42 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "26/01/29 15:41:51 WARN DAGScheduler: Broadcasting large task binary with size 1222.0 KiB\n",
      "26/01/29 15:41:53 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "26/01/29 15:41:55 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "26/01/29 15:41:58 WARN DAGScheduler: Broadcasting large task binary with size 1355.7 KiB\n",
      "26/01/29 15:41:58 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "26/01/29 15:42:02 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "26/01/29 15:42:02 WARN DAGScheduler: Broadcasting large task binary with size 15.7 MiB\n",
      "26/01/29 15:42:07 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "26/01/29 15:42:08 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n",
      "26/01/29 15:42:19 WARN DAGScheduler: Broadcasting large task binary with size 1212.3 KiB\n",
      "26/01/29 15:42:21 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "26/01/29 15:42:25 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "26/01/29 15:42:29 WARN DAGScheduler: Broadcasting large task binary with size 1390.4 KiB\n",
      "26/01/29 15:42:30 WARN DAGScheduler: Broadcasting large task binary with size 8.6 MiB\n",
      "26/01/29 15:42:34 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "26/01/29 15:42:35 WARN DAGScheduler: Broadcasting large task binary with size 16.2 MiB\n",
      "26/01/29 15:42:40 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "26/01/29 15:42:42 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RF Accuracy: 0.7365146142849045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/29 15:42:45 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n",
      "[Stage 361:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RF F1 Score: 0.7225512663308105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [50, 100])\n",
    "    .addGrid(rf.maxDepth, [5, 10])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator_acc,   # accuracy for model selection\n",
    "    numFolds=3,\n",
    "    parallelism=2\n",
    ")\n",
    "\n",
    "# (Recommended) cache training data for CV\n",
    "train_df.cache()\n",
    "train_df.count()\n",
    "\n",
    "cv_model = cv.fit(train_df)\n",
    "\n",
    "rf_preds = cv_model.bestModel.transform(test_df)\n",
    "\n",
    "print(f\"Best RF Accuracy: {evaluator_acc.evaluate(rf_preds)}\")\n",
    "print(f\"Best RF F1 Score: {evaluator_f1.evaluate(rf_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65035806",
   "metadata": {},
   "source": [
    "Note: SparkXGBClassifier cannot be used with ParamGridBuilder or CrossValidator. That's why we are using Random Forest Classifier only for hyperparameter tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
