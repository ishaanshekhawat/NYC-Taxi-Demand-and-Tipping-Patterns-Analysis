{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56762fb5-7b4f-4a21-8f0a-3da9807d993e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Config loaded\n",
      "ℹ No existing Spark session to stop\n",
      "Creating Spark session: NYC Taxi EDA\n",
      "✓ Spark session created successfully (version 3.5.0)\n",
      "Configuring Hadoop for MinIO...\n",
      "✓ Spark configured for environment: development\n",
      "✓ Using MinIO endpoint: http://minio:9000\n",
      "✓ Reading from bucket: nyc-taxi\n",
      "\n",
      "==================================================\n",
      "✓ Spark Session Ready\n",
      "==================================================\n",
      "==================================================\n",
      "Current Configuration:\n",
      "==================================================\n",
      "App Name: NYC Taxi EDA\n",
      "Environment: development\n",
      "MinIO Endpoint: http://minio:9000\n",
      "S3 Bucket: nyc-taxi\n",
      "Spark Driver Memory: 3g\n",
      "Spark Executor Memory: 3g\n",
      "Spark Executor Instances: 3\n",
      "Spark Executor Cores: 2\n",
      "Log Level: INFO\n",
      "Log File: eda.log\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import importlib.util\n",
    "import os\n",
    "\n",
    "# ========== LOAD CONFIG FIRST ==========\n",
    "src_path = os.path.join(os.path.dirname(os.getcwd()), 'src')\n",
    "config_file = os.path.join(src_path, 'config.py')\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"config\", config_file)\n",
    "config_module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(config_module)\n",
    "\n",
    "Config = config_module.Config\n",
    "print(\"✓ Config loaded\")\n",
    "\n",
    "# ========== STOP EXISTING SPARK ==========\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"✓ Stopped existing Spark session\")\n",
    "except:\n",
    "    print(\"ℹ No existing Spark session to stop\")\n",
    "\n",
    "# ========== CREATE SPARK SESSION ==========\n",
    "print(f\"Creating Spark session: {Config.APP_NAME}\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(Config.APP_NAME) \\\n",
    "    .config(\"spark.driver.memory\", Config.SPARK_DRIVER_MEMORY) \\\n",
    "    .config(\"spark.executor.memory\", Config.SPARK_EXECUTOR_MEMORY) \\\n",
    "    .config(\"spark.executor.instances\", Config.SPARK_EXECUTOR_INSTANCES) \\\n",
    "    .config(\"spark.executor.cores\", Config.SPARK_EXECUTOR_CORES) \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✓ Spark session created successfully (version {spark.version})\")\n",
    "\n",
    "# ========== CONFIGURE HADOOP FOR MINIO ==========\n",
    "print(\"Configuring Hadoop for MinIO...\")\n",
    "\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", Config.MINIO_ENDPOINT)\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", Config.MINIO_ACCESS_KEY)\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", Config.MINIO_SECRET_KEY)\n",
    "hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoop_conf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "print(f\"✓ Spark configured for environment: {Config.ENVIRONMENT}\")\n",
    "print(f\"✓ Using MinIO endpoint: {Config.MINIO_ENDPOINT}\")\n",
    "print(f\"✓ Reading from bucket: {Config.S3_BUCKET_NAME}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"✓ Spark Session Ready\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Display configuration\n",
    "Config.display_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b518d18c-6b51-4ae6-98ec-82600fd9cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"s3a://nyc-taxi/Clean_DF/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "984a11c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, tpep_pickup_datetime: timestamp_ntz, tpep_dropoff_datetime: timestamp_ntz, passenger_count: bigint, trip_distance: double, RatecodeID: bigint, store_and_fwd_flag: string, PULocationID: int, DOLocationID: int, payment_type: bigint, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, congestion_surcharge: double, Airport_fee: double, cbd_congestion_fee: double]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.coalesce(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91370829-9a3f-49c0-b312-8140436c4818",
   "metadata": {},
   "source": [
    "### Tip Estimation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b662bcd-2aea-41a9-9eb6-7c8804b8bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    hour, dayofweek, month, when, sqrt, \n",
    "    unix_timestamp, cos, sin, radians, concat\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "df_tips1 = df.select(\"tip_amount\", \"fare_amount\", \"trip_distance\", \"payment_type\", \"PULocationID\", \"DOLocationID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\")\n",
    "\n",
    "df_tips1 = df_tips1.withColumn(\"pickup_hour\", hour(\"tpep_pickup_datetime\")) \\\n",
    "       .withColumn(\"pickup_day\", dayofweek(\"tpep_pickup_datetime\")) \\\n",
    "       .withColumn(\"pickup_month\", month(\"tpep_pickup_datetime\")) \\\n",
    "       .withColumn(\"is_weekend\", when(col(\"pickup_day\").isin([1, 7]), 1).otherwise(0)) \\\n",
    "       .withColumn(\"is_rush_hour\", when(\n",
    "           ((col(\"pickup_hour\") >= 6) & (col(\"pickup_hour\") <= 10)) | \n",
    "           ((col(\"pickup_hour\") >= 16) & (col(\"pickup_hour\") <= 20)), 1\n",
    "       ).otherwise(0)) \\\n",
    "       .withColumn(\"is_night\", when(\n",
    "           ((col(\"pickup_hour\") >= 20) | (col(\"pickup_hour\") <= 6)), 1\n",
    "       ).otherwise(0)) \\\n",
    "       .withColumn(\"tip_percent\", (col(\"tip_amount\") / col(\"fare_amount\")) * 100) \\\n",
    "       .withColumn(\"PU_DO_pair\", concat(col(\"PULocationID\").cast(\"string\"), lit(\"_\"), col(\"DOLocationID\").cast(\"string\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43d51f5f-024b-41dd-a510-92a52ebdeb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+------------+------------+------------+--------------------+---------------------+-----------+----------+------------+----------+------------+--------+------------------+----------+\n",
      "|tip_amount|fare_amount|trip_distance|payment_type|PULocationID|DOLocationID|tpep_pickup_datetime|tpep_dropoff_datetime|pickup_hour|pickup_day|pickup_month|is_weekend|is_rush_hour|is_night|       tip_percent|PU_DO_pair|\n",
      "+----------+-----------+-------------+------------+------------+------------+--------------------+---------------------+-----------+----------+------------+----------+------------+--------+------------------+----------+\n",
      "|       0.0|        9.3|         0.97|           2|         161|         141| 2023-01-01 00:32:10|  2023-01-01 00:40:36|          0|         1|           1|         1|           0|       0|               0.0|   161_141|\n",
      "|       4.0|        7.9|          1.1|           1|          43|         237| 2023-01-01 00:55:08|  2023-01-01 01:01:27|          0|         1|           1|         1|           0|       0|50.632911392405056|    43_237|\n",
      "|      15.0|       14.9|         2.51|           1|          48|         238| 2023-01-01 00:25:04|  2023-01-01 00:37:49|          0|         1|           1|         1|           0|       0|100.67114093959731|    48_238|\n",
      "|       0.0|       12.1|          1.9|           1|         138|           7| 2023-01-01 00:03:48|  2023-01-01 00:13:25|          0|         1|           1|         1|           0|       0|               0.0|     138_7|\n",
      "|      3.28|       11.4|         1.43|           1|         107|          79| 2023-01-01 00:10:29|  2023-01-01 00:21:19|          0|         1|           1|         1|           0|       0|28.771929824561397|    107_79|\n",
      "|      10.0|       12.8|         1.84|           1|         161|         137| 2023-01-01 00:50:34|  2023-01-01 01:02:52|          0|         1|           1|         1|           0|       0|            78.125|   161_137|\n",
      "|      3.42|       12.1|         1.66|           1|         239|         143| 2023-01-01 00:09:22|  2023-01-01 00:19:49|          0|         1|           1|         1|           0|       0|28.264462809917358|   239_143|\n",
      "|     10.74|       45.7|         11.7|           1|         142|         200| 2023-01-01 00:27:12|  2023-01-01 00:49:56|          0|         1|           1|         1|           0|       0|23.501094091903717|   142_200|\n",
      "|      5.68|       17.7|         2.95|           1|         164|         236| 2023-01-01 00:21:44|  2023-01-01 00:36:40|          0|         1|           1|         1|           0|       0|32.090395480225986|   164_236|\n",
      "|       0.0|       14.9|         3.01|           2|         141|         107| 2023-01-01 00:39:42|  2023-01-01 00:50:36|          0|         1|           1|         1|           0|       0|               0.0|   141_107|\n",
      "|      3.28|       11.4|          1.8|           1|         234|          68| 2023-01-01 00:53:01|  2023-01-01 01:01:45|          0|         1|           1|         1|           0|       0|28.771929824561397|    234_68|\n",
      "|      7.75|       33.8|          7.3|           1|          79|         264| 2023-01-01 00:43:37|  2023-01-01 01:17:18|          0|         1|           1|         1|           0|       0| 22.92899408284024|    79_264|\n",
      "|      6.22|       26.1|         3.23|           1|         164|         143| 2023-01-01 00:34:44|  2023-01-01 01:04:25|          0|         1|           1|         1|           0|       0| 23.83141762452107|   164_143|\n",
      "|     13.26|       44.3|        11.43|           1|         138|          33| 2023-01-01 00:09:29|  2023-01-01 00:29:23|          0|         1|           1|         1|           0|       0| 29.93227990970655|    138_33|\n",
      "|      4.04|       17.7|         2.95|           1|          33|          61| 2023-01-01 00:33:53|  2023-01-01 00:49:15|          0|         1|           1|         1|           0|       0|22.824858757062145|     33_61|\n",
      "|      1.25|       10.0|         1.52|           1|          79|         186| 2023-01-01 00:13:04|  2023-01-01 00:22:10|          0|         1|           1|         1|           0|       0|              12.5|    79_186|\n",
      "|      4.96|       19.8|         2.23|           1|          90|          48| 2023-01-01 00:45:11|  2023-01-01 01:07:39|          0|         1|           1|         1|           0|       0|25.050505050505052|     90_48|\n",
      "|       4.0|       20.5|          4.5|           1|         113|         255| 2023-01-01 00:04:33|  2023-01-01 00:19:22|          0|         1|           1|         1|           0|       0| 19.51219512195122|   113_255|\n",
      "|       0.0|        8.6|          1.2|           2|         237|         239| 2023-01-01 00:03:36|  2023-01-01 00:09:36|          0|         1|           1|         1|           0|       0|               0.0|   237_239|\n",
      "|       0.0|       15.6|          2.5|           2|         143|         229| 2023-01-01 00:15:23|  2023-01-01 00:29:41|          0|         1|           1|         1|           0|       0|               0.0|   143_229|\n",
      "+----------+-----------+-------------+------------+------------+------------+--------------------+---------------------+-----------+----------+------------+----------+------------+--------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tips1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7461ba-72a0-4c90-a51b-045dd6bcd8c0",
   "metadata": {},
   "source": [
    "The payment_type 2 (Cash) consistently shows a tip_amount of 0.0. This isn't necessarily because passengers didn't tip, but because cash tips aren't recorded in the system. Likewise, other payment systems are also not much helpful here. Therefore, we need to filter them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c761e401-1795-453b-aba0-301e2fa54740",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tips1 = df_tips1.filter(col(\"payment_type\") == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c3ebde9-51e3-45fa-a339-91c4b88aa6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79757391"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tips1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05810cb-91b9-4615-ac2b-1350827c6d52",
   "metadata": {},
   "source": [
    "Before moving ahead with the columns to aggregate data based on, let's take a look at how much they effect the avg tip percent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6714388a-8127-49b9-bb43-2a7f1b255126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|is_night|  avg(tip_percent)|\n",
      "+--------+------------------+\n",
      "|       1|25.652914971662863|\n",
      "|       0| 25.45770167729256|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tips1.groupBy('is_night').avg('tip_percent').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e93558fc-7b0c-4310-95d7-cb584a3cc4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|is_rush_hour|  avg(tip_percent)|\n",
      "+------------+------------------+\n",
      "|           1|25.954784225899857|\n",
      "|           0|25.088459399006698|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tips1.groupBy('is_rush_hour').avg('tip_percent').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8606a9f8-6911-4a71-a4a0-2e7e8be1a1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|is_weekend|  avg(tip_percent)|\n",
      "+----------+------------------+\n",
      "|         1|24.828555103618953|\n",
      "|         0| 25.76748909972411|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tips1.groupBy('is_weekend').avg('tip_percent').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "199981a7-adab-4ee8-90ce-18c866621736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|pickup_month|  avg(tip_percent)|\n",
      "+------------+------------------+\n",
      "|          12|25.094430361089938|\n",
      "|           1| 26.17986607994806|\n",
      "|          10|  25.1045627233786|\n",
      "|           2|25.640637133914066|\n",
      "|           9|24.462165139128004|\n",
      "|          11|25.838064580083504|\n",
      "|           6| 25.39009100305301|\n",
      "|           5|25.176676371847293|\n",
      "|           4|25.833125426733066|\n",
      "|           8|25.085676363384053|\n",
      "|           7| 26.04541650619972|\n",
      "|           3|25.861159713259585|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tips1.groupBy('pickup_month').avg('tip_percent').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c809569-c391-4650-97fe-af0fb89a0d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|pickup_day|  avg(tip_percent)|\n",
      "+----------+------------------+\n",
      "|         1| 24.68717291108004|\n",
      "|         6|25.410508234716264|\n",
      "|         3| 25.90459606725041|\n",
      "|         5|26.139571511754323|\n",
      "|         4| 25.62878622528968|\n",
      "|         7| 24.94809902633729|\n",
      "|         2|25.726623277243664|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tips1.groupBy('pickup_day').avg('tip_percent').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a71e989a-f309-4e6f-b976-f48571ab1705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|pickup_hour|  avg(tip_percent)|\n",
      "+-----------+------------------+\n",
      "|         12|24.321234681388066|\n",
      "|         22|25.501986294332305|\n",
      "|          1|25.076702127132897|\n",
      "|         13|24.750965118406594|\n",
      "|          6|26.507472521438352|\n",
      "|         16|26.181031922599008|\n",
      "|          3|25.194811575017702|\n",
      "|         20|25.841446947656554|\n",
      "|          5| 23.15155688651097|\n",
      "|         19| 27.32282026567797|\n",
      "|         15|24.898389362389395|\n",
      "|          9|  24.4288315248703|\n",
      "|         17|26.509070491832205|\n",
      "|          4|28.874924322051587|\n",
      "|          8| 24.52653518680321|\n",
      "|         23|25.519452570890802|\n",
      "|          7|24.571234304755865|\n",
      "|         10|  24.6742172351314|\n",
      "|         21| 25.66991246508552|\n",
      "|         11|25.246506511408157|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tips1.groupBy('pickup_hour').avg('tip_percent').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5483f58d-78b4-42a2-b055-a4b1e970db39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       effect_size|\n",
      "+------------------+\n",
      "|5.7233674355406166|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|       effect_size|\n",
      "+------------------+\n",
      "|1.7177009408200554|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|       effect_size|\n",
      "+------------------+\n",
      "|0.9389339961051562|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|       effect_size|\n",
      "+------------------+\n",
      "|0.8663248268931589|\n",
      "+------------------+\n",
      "\n",
      "+-------------------+\n",
      "|        effect_size|\n",
      "+-------------------+\n",
      "|0.19521329437030133|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, max as spark_max, min as spark_min\n",
    "\n",
    "def effect_size(df, group_col, target=\"tip_percent\"):\n",
    "    ag = df.groupBy(group_col).agg(avg(target).alias(\"avg_tip\"))\n",
    "    return ag.agg((spark_max(\"avg_tip\") - spark_min(\"avg_tip\")).alias(\"effect_size\"))\n",
    "\n",
    "# Examples\n",
    "effect_size(df_tips1, \"pickup_hour\").show()\n",
    "effect_size(df_tips1, \"pickup_month\").show()\n",
    "effect_size(df_tips1, \"is_weekend\").show()\n",
    "effect_size(df_tips1, \"is_rush_hour\").show()\n",
    "effect_size(df_tips1, \"is_night\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79454c31-8087-4645-988f-84603c9e25f8",
   "metadata": {},
   "source": [
    "Turns out:\n",
    "\n",
    "1. pickup_hour shows the largest spread → strong signal\n",
    "2. pickup_month shows a small-to-moderate spread → useful but weaker\n",
    "3. is_weekend / is_rush_hour / is_night → small effects, still useful as filters/controls but not primary drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0cd11493-59cb-4772-9223-071b93f1009b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+------------+----------+------------+---------------+--------------+-----+\n",
      "|PULocationID|DOLocationID|pickup_hour|pickup_month|is_weekend|is_rush_hour|avg_tip_percent|avg_tip_amount|rides|\n",
      "+------------+------------+-----------+------------+----------+------------+---------------+--------------+-----+\n",
      "|          50|         186|          0|           1|         1|           0|          27.05|           2.7|    9|\n",
      "|         236|         262|          0|           1|         1|           0|          44.68|          3.17|   45|\n",
      "|          48|         260|          0|           1|         1|           0|          24.25|          7.15|    6|\n",
      "|          90|         142|          0|           1|         1|           0|          21.21|          3.42|   45|\n",
      "|           7|         116|          0|           1|         1|           0|          33.24|          9.14|    1|\n",
      "|         229|         162|          1|           1|         1|           0|          32.86|          1.81|   14|\n",
      "|          50|          68|          1|           1|         1|           0|          24.32|          1.98|   22|\n",
      "|         142|         161|          1|           1|         1|           0|          24.21|          2.35|    7|\n",
      "|          48|         249|          1|           1|         1|           0|          25.14|          3.33|   89|\n",
      "|         137|         237|          1|           1|         1|           0|          23.44|          2.52|   15|\n",
      "|         162|         137|          1|           1|         1|           0|          32.83|          2.28|   14|\n",
      "|         125|          45|          1|           1|         1|           0|          28.93|          3.18|    4|\n",
      "|         144|         246|          1|           1|         1|           0|           16.7|          2.84|   31|\n",
      "|          80|         112|          2|           1|         1|           0|          52.42|          3.21|    3|\n",
      "|         249|         142|          2|           1|         1|           0|          22.53|          4.04|   62|\n",
      "|         233|         249|          2|           1|         1|           0|          24.51|          4.26|    4|\n",
      "|         249|         211|          2|           1|         1|           0|          28.98|          2.25|   62|\n",
      "|         255|           4|          2|           1|         1|           0|          17.93|          3.32|    4|\n",
      "|         137|          13|          2|           1|         1|           0|          21.88|          4.95|    5|\n",
      "|         231|         211|          2|           1|         1|           0|           39.2|          2.23|   24|\n",
      "+------------+------------+-----------+------------+----------+------------+---------------+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, count, round\n",
    "\n",
    "df_tips1.groupBy(\n",
    "    \"PULocationID\", \"DOLocationID\", \"pickup_hour\", \"pickup_month\", \"is_weekend\", \"is_rush_hour\"\n",
    ").agg(\n",
    "    round(avg(\"tip_percent\"), 2).alias(\"avg_tip_percent\"),\n",
    "    round(avg(\"tip_amount\"), 2).alias(\"avg_tip_amount\"),\n",
    "    count(\"*\").alias(\"rides\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab41cec-7359-4d4e-a94a-b37bb375142d",
   "metadata": {},
   "source": [
    "Many of the rides here too less to give us a good average of tip percentage. Therefore we have created a function below that keeps dropping the lowest priority aggregations until the count of rides reaches at least 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e270fe73-f880-4e10-a68f-7c35f1fa4f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, count, round\n",
    "\n",
    "def get_avg_tip_percent_with_fallback(\n",
    "    df,\n",
    "    pu_location_id,\n",
    "    do_location_id,\n",
    "    pickup_hour,\n",
    "    pickup_month,\n",
    "    is_weekend,\n",
    "    is_rush_hour,\n",
    "    is_night,\n",
    "    min_rides=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns avg_tip_percent using fallback logic if rides are low.\n",
    "    \n",
    "    Priority of dropping filters (lowest priority dropped first):\n",
    "    is_night -> is_rush_hour -> is_weekend -> pickup_month\n",
    "    \n",
    "    Output columns:\n",
    "    avg_tip_percent, avg_tip_amount, rides_used\n",
    "    \"\"\"\n",
    "\n",
    "    # Each step contains filters to apply\n",
    "    # We start with the most strict filters\n",
    "    fallback_steps = [\n",
    "        {\n",
    "            \"fallback_level\": \"FULL (PU+DO+hour+month+weekend+rush+night)\",\n",
    "            \"filters\": [\n",
    "                col(\"PULocationID\") == pu_location_id,\n",
    "                col(\"DOLocationID\") == do_location_id,\n",
    "                col(\"pickup_hour\") == pickup_hour,\n",
    "                col(\"pickup_month\") == pickup_month,\n",
    "                col(\"is_weekend\") == is_weekend,\n",
    "                col(\"is_rush_hour\") == is_rush_hour,\n",
    "                col(\"is_night\") == is_night,\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"fallback_level\": \"DROP is_night\",\n",
    "            \"filters\": [\n",
    "                col(\"PULocationID\") == pu_location_id,\n",
    "                col(\"DOLocationID\") == do_location_id,\n",
    "                col(\"pickup_hour\") == pickup_hour,\n",
    "                col(\"pickup_month\") == pickup_month,\n",
    "                col(\"is_weekend\") == is_weekend,\n",
    "                col(\"is_rush_hour\") == is_rush_hour,\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"fallback_level\": \"DROP is_night + is_rush_hour\",\n",
    "            \"filters\": [\n",
    "                col(\"PULocationID\") == pu_location_id,\n",
    "                col(\"DOLocationID\") == do_location_id,\n",
    "                col(\"pickup_hour\") == pickup_hour,\n",
    "                col(\"pickup_month\") == pickup_month,\n",
    "                col(\"is_weekend\") == is_weekend,\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"fallback_level\": \"DROP is_night + is_rush_hour + is_weekend\",\n",
    "            \"filters\": [\n",
    "                col(\"PULocationID\") == pu_location_id,\n",
    "                col(\"DOLocationID\") == do_location_id,\n",
    "                col(\"pickup_hour\") == pickup_hour,\n",
    "                col(\"pickup_month\") == pickup_month,\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"fallback_level\": \"DROP is_night + is_rush_hour + is_weekend + pickup_month\",\n",
    "            \"filters\": [\n",
    "                col(\"PULocationID\") == pu_location_id,\n",
    "                col(\"DOLocationID\") == do_location_id,\n",
    "                col(\"pickup_hour\") == pickup_hour,\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for step in fallback_steps:\n",
    "        # Apply filters\n",
    "        filtered_df = df\n",
    "        for f in step[\"filters\"]:\n",
    "            filtered_df = filtered_df.filter(f)\n",
    "\n",
    "        # Count rides\n",
    "        rides = filtered_df.count()\n",
    "\n",
    "        # If enough rides, calculate avg tip percent and return\n",
    "        if rides >= min_rides:\n",
    "            result = filtered_df.agg(\n",
    "                round(avg(\"tip_percent\"), 2).alias(\"avg_tip_percent\"),\n",
    "                round(avg(\"tip_amount\"), 2).alias(\"avg_tip_amount\"),\n",
    "                count(\"*\").alias(\"rides_used\")\n",
    "            )\n",
    "\n",
    "            return result\n",
    "\n",
    "    # If even last fallback doesn't reach min rides, return whatever exists (even if low)\n",
    "    from pyspark.sql.functions import lit\n",
    "    last_df = df.filter(\n",
    "        (col(\"PULocationID\") == pu_location_id) &\n",
    "        (col(\"DOLocationID\") == do_location_id) &\n",
    "        (col(\"pickup_hour\") == pickup_hour)\n",
    "    )\n",
    "\n",
    "    return last_df.agg(\n",
    "        round(avg(\"tip_percent\"), 2).alias(\"avg_tip_percent\"),\n",
    "        round(avg(\"tip_amount\"), 2).alias(\"avg_tip_amount\"),\n",
    "        count(\"*\").alias(\"rides_used\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45cce577-0b95-43b8-a9c5-e10c8405ef2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+--------------+\n",
      "|avg_tip_percent|rides_used|avg_tip_amount|\n",
      "+---------------+----------+--------------+\n",
      "|27.23          |974       |2.91          |\n",
      "+---------------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing the function for a random set of values.\n",
    "\n",
    "result_df = get_avg_tip_percent_with_fallback(\n",
    "    df=df_tips1,\n",
    "    pu_location_id=161,\n",
    "    do_location_id=141,\n",
    "    pickup_hour=20,\n",
    "    pickup_month=1,\n",
    "    is_weekend=1,\n",
    "    is_rush_hour=0,\n",
    "    is_night=1,\n",
    "    min_rides=100\n",
    ")\n",
    "\n",
    "result_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64fd30b7-ab5e-493c-a01c-b7df64cbf266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79757391"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tips1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e6ef30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count, round\n",
    "\n",
    "streamlit_dataset_1 = df_tips1.groupBy(\n",
    "    \"PULocationID\", \"DOLocationID\", \"pickup_hour\", \"pickup_month\", \"is_weekend\"\n",
    ").agg(\n",
    "    round(avg(\"tip_percent\"), 2).alias(\"avg_tip_percent\"),\n",
    "    round(avg(\"tip_amount\"), 2).alias(\"avg_tip_amount\"),\n",
    "    round(avg(\"fare_amount\"), 2).alias(\"avg_fare_amount\"),\n",
    "    count(\"*\").alias(\"rides\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f36a29c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this for the tip estimation model\n",
    "streamlit_dataset_1.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"s3a://nyc-taxi/Streamlit_Tips_Estimate_DF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52992a0a-f712-43eb-90c8-4aa49ed00f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df_tips1 for the nearby hotspots suggestion model\n",
    "df_tips1.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"s3a://nyc-taxi/Nearby_Hotspots_DF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2738e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sample dataframe to be used to create the ML model for tip amount range prediction\n",
    "tip_prediction_model_df = df_tips1.sample(fraction=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1a1fae6-8696-4698-963a-882c14d531cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this for the tip amount range prediction\n",
    "tip_prediction_model_df.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"s3a://nyc-taxi/Tip_Prediction_Model_DF\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
