version: '3.8'

services:
  minio:
    image: minio/minio:latest
    container_name: minio
    restart: unless-stopped
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin123
    ports:
      - "9000:9000"   # S3 API
      - "9001:9001"   # MinIO Console
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    networks:
      - appnet
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3

  jupyter:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyter-pyspark
    restart: unless-stopped
    depends_on:
      - minio
    ports:
      - "8888:8888"
    volumes:
      - ./work:/home/jovyan/work
    environment:
      JUPYTER_TOKEN: mytoken                  # change this or use env file
      MINIO_ENDPOINT: http://minio:9000
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin123
      PYSPARK_SUBMIT_ARGS: >
        --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.367 pyspark-shell
    command: start-notebook.sh --NotebookApp.token='mytoken' --NotebookApp.ip='0.0.0.0' --NotebookApp.allow_origin='*'
    networks:
      - appnet

volumes:
  minio_data:

networks:
  appnet:
    driver: bridge

# Notes:
# - Change credentials before using in production. Consider using an external .env file or Docker secrets.
# - MinIO console is available on http://localhost:9001 (UI) and S3-compatible API on http://localhost:9000
# - Jupyter Notebook will be available on http://localhost:8888 with the token set above.
# - The Jupyter container has the MINIO credentials as env vars so notebooks can use boto3 or Spark's s3a.
#
# Example PySpark snippet (run inside the Jupyter notebook) to access MinIO via s3a:
# from pyspark.sql import SparkSession
# spark = SparkSession.builder.appName('minio-test').getOrCreate()
# hadoop_conf = spark._jsc.hadoopConfiguration()
# hadoop_conf.set('fs.s3a.endpoint', 'http://minio:9000')
# hadoop_conf.set('fs.s3a.access.key', 'minioadmin')
# hadoop_conf.set('fs.s3a.secret.key', 'minioadmin123')
# hadoop_conf.set('fs.s3a.path.style.access', 'true')
# hadoop_conf.set('fs.s3a.connection.ssl.enabled', 'false')
# df = spark.read.csv('s3a://my-bucket/path/to/file.csv', header=True)
# df.show()
